{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.x\n",
    "\n",
    "1) Подготовка данных\n",
    "\n",
    "2) Использование Keras Model API\n",
    "\n",
    "3) Использование Keras Sequential + Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения лабораторной работы необходимо установить tensorflow версии 2.0 или выше .\n",
    "\n",
    "Рекомендуется использовать возможности Colab'а по обучению моделей на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "Загрузите набор данных из предыдущей лабораторной работы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Train data shape:  (49000, 28, 28)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 28, 28)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    data = tf.keras.datasets.mnist.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = data\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 28, 28) (64,)\n",
      "1 (64, 28, 28) (64,)\n",
      "2 (64, 28, 28) (64,)\n",
      "3 (64, 28, 28) (64,)\n",
      "4 (64, 28, 28) (64,)\n",
      "5 (64, 28, 28) (64,)\n",
      "6 (64, 28, 28) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Keras Model Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Для реализации собственной модели с помощью Keras Model Subclassing API необходимо выполнить следующие шаги:\n",
    "\n",
    "1) Определить новый класс, который является наследником tf.keras.Model.\n",
    "\n",
    "2) В методе __init__() определить все необходимые слои из модуля tf.keras.layer\n",
    "\n",
    "3) Реализовать прямой проход в методе call() на основе слоев, объявленных в __init__()\n",
    "\n",
    "Ниже приведен пример использования keras API для определения двухслойной полносвязной сети. \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "device = '/device:GPU:0'\n",
    "\n",
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте трехслойную CNN для вашей задачи классификации. \n",
    "\n",
    "Архитектура сети:\n",
    "    \n",
    "1. Сверточный слой (5 x 5 kernels, zero-padding = 'same')\n",
    "2. Функция активации ReLU \n",
    "3. Сверточный слой (3 x 3 kernels, zero-padding = 'same')\n",
    "4. Функция активации ReLU \n",
    "5. Полносвязный слой \n",
    "6. Функция активации Softmax \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        kernel_initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # First layer\n",
    "        self.conv_2d_1 = tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='same', kernel_initializer=kernel_initializer, activation='relu')\n",
    "        # Second layer\n",
    "        self.conv_2d_2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer, activation='relu')\n",
    "        # Output layer\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=kernel_initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv_2d_1(x)\n",
    "        x = self.conv_2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        scores = self.dense(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\src\\python\\DL_Course\\venv\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример реализации процесса обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 200\n",
    "\n",
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "\n",
    "        \n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.3293864727020264, Accuracy: 1.5625, Val Loss: 2.467919111251831, Val Accuracy: 23.399999618530273\n",
      "Iteration 200, Epoch 1, Loss: 0.5054786801338196, Accuracy: 84.91915130615234, Val Loss: 0.4547783434391022, Val Accuracy: 85.9000015258789\n",
      "Iteration 400, Epoch 1, Loss: 0.40421444177627563, Accuracy: 88.03382110595703, Val Loss: 0.36935093998908997, Val Accuracy: 89.20000457763672\n",
      "Iteration 600, Epoch 1, Loss: 0.3589380383491516, Accuracy: 89.40827941894531, Val Loss: 0.34514451026916504, Val Accuracy: 90.30000305175781\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите трехслойную CNN. В tf.keras.optimizers.SGD укажите Nesterov momentum = 0.9 . \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "Значение accuracy на валидационной выборке после 1 эпохи обучения должно быть > 50% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\src\\python\\DL_Course\\venv\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.64253306388855, Accuracy: 15.625, Val Loss: 2.33231258392334, Val Accuracy: 16.100000381469727\n",
      "Iteration 200, Epoch 1, Loss: 0.30792173743247986, Accuracy: 90.63277435302734, Val Loss: 0.23065949976444244, Val Accuracy: 92.0999984741211\n",
      "Iteration 400, Epoch 1, Loss: 0.21408917009830475, Accuracy: 93.52790069580078, Val Loss: 0.1509048491716385, Val Accuracy: 95.0\n",
      "Iteration 600, Epoch 1, Loss: 0.1777547299861908, Accuracy: 94.68594360351562, Val Loss: 0.16283921897411346, Val Accuracy: 94.5999984741211\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate, 0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Sequential API для реализации последовательных моделей.\n",
    "\n",
    "Пример для полносвязной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.1466312408447266, Accuracy: 1.5625, Val Loss: 2.6549553871154785, Val Accuracy: 22.0\n",
      "Iteration 200, Epoch 1, Loss: 0.5178578495979309, Accuracy: 84.68594360351562, Val Loss: 0.45274806022644043, Val Accuracy: 86.0999984741211\n",
      "Iteration 400, Epoch 1, Loss: 0.41404861211776733, Accuracy: 87.81951904296875, Val Loss: 0.36187389492988586, Val Accuracy: 89.4000015258789\n",
      "Iteration 600, Epoch 1, Loss: 0.36701303720474243, Accuracy: 89.22628784179688, Val Loss: 0.33457234501838684, Val Accuracy: 90.69999694824219\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (28, 28, 1)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный менее гибкий способ обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 12s 15ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.3018 - val_sparse_categorical_accuracy: 0.9190\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2320 - sparse_categorical_accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23197638988494873, 0.9330000281333923]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите реализацию трехслойной CNN с помощью tf.keras.Sequential API . Обучите модель двумя способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.4916000366210938, Accuracy: 17.1875, Val Loss: 2.40303373336792, Val Accuracy: 15.399999618530273\n",
      "Iteration 200, Epoch 1, Loss: 0.5852739214897156, Accuracy: 82.40827178955078, Val Loss: 0.4662199914455414, Val Accuracy: 85.29999542236328\n",
      "Iteration 400, Epoch 1, Loss: 0.4268709123134613, Accuracy: 87.25841522216797, Val Loss: 0.29291850328445435, Val Accuracy: 90.69999694824219\n",
      "Iteration 600, Epoch 1, Loss: 0.3553744852542877, Accuracy: 89.48627471923828, Val Loss: 0.2744094431400299, Val Accuracy: 91.5\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    channel_1, channel_2, num_classes = 28, 14, 10\n",
    "    kernel_initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='same', kernel_initializer=kernel_initializer, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=kernel_initializer)\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 29s 38ms/step - loss: 0.2931 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.1641 - val_sparse_categorical_accuracy: 0.9450\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.1044 - sparse_categorical_accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10435730218887329, 0.9700999855995178]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Keras Functional API\n",
    "\n",
    "Для реализации более сложных архитектур сети с несколькими входами/выходами, повторным использованием слоев, \"остаточными\" связями (residual connections) необходимо явно указать входные и выходные тензоры. \n",
    "\n",
    "Ниже представлен пример для полносвязной сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu', \n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.031647205352783, Accuracy: 7.8125, Val Loss: 2.748664379119873, Val Accuracy: 17.5\n",
      "Iteration 200, Epoch 1, Loss: 0.5168659687042236, Accuracy: 84.48383331298828, Val Loss: 0.4636557102203369, Val Accuracy: 86.0999984741211\n",
      "Iteration 400, Epoch 1, Loss: 0.4133908748626709, Accuracy: 87.597412109375, Val Loss: 0.38099777698516846, Val Accuracy: 89.0999984741211\n",
      "Iteration 600, Epoch 1, Loss: 0.3642720878124237, Accuracy: 89.16129302978516, Val Loss: 0.3371207118034363, Val Accuracy: 90.80000305175781\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с архитектурой сверточной сети. Для вашего набора данных вам необходимо получить как минимум 70% accuracy на валидационной выборке за 10 эпох обучения. Опишите все эксперименты и сделайте выводы (без выполнения данного пункта работы приниматься не будут). \n",
    "\n",
    "Эспериментируйте с архитектурой, гиперпараметрами, функцией потерь, регуляризацией, методом оптимизации.  \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\src\\python\\DL_Course\\venv\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.576293706893921, Accuracy: 7.8125, Val Loss: 2.8266239166259766, Val Accuracy: 19.5\n",
      "Iteration 700, Epoch 1, Loss: 0.16973356902599335, Accuracy: 94.8332748413086, Val Loss: 0.13477586209774017, Val Accuracy: 95.70000457763672\n",
      "Iteration 1400, Epoch 2, Loss: 0.05423677712678909, Accuracy: 98.30462646484375, Val Loss: 0.10108760744333267, Val Accuracy: 96.9000015258789\n",
      "Iteration 2100, Epoch 3, Loss: 0.027833860367536545, Accuracy: 99.10204315185547, Val Loss: 0.14685797691345215, Val Accuracy: 95.70000457763672\n",
      "Iteration 2800, Epoch 4, Loss: 0.01835336536169052, Accuracy: 99.3694076538086, Val Loss: 0.1152573823928833, Val Accuracy: 96.80000305175781\n",
      "Iteration 3500, Epoch 5, Loss: 0.016301140189170837, Accuracy: 99.38143157958984, Val Loss: 0.16427810490131378, Val Accuracy: 95.80000305175781\n",
      "Iteration 4200, Epoch 6, Loss: 0.010541919618844986, Accuracy: 99.62516784667969, Val Loss: 0.15349283814430237, Val Accuracy: 96.80000305175781\n",
      "Iteration 4900, Epoch 7, Loss: 0.008282002061605453, Accuracy: 99.68238067626953, Val Loss: 0.15313021838665009, Val Accuracy: 96.9000015258789\n",
      "Iteration 5600, Epoch 8, Loss: 0.008748876862227917, Accuracy: 99.7319564819336, Val Loss: 0.16050949692726135, Val Accuracy: 97.0999984741211\n",
      "Iteration 6300, Epoch 9, Loss: 0.010087473317980766, Accuracy: 99.69291687011719, Val Loss: 0.1403643786907196, Val Accuracy: 97.19999694824219\n",
      "Iteration 7000, Epoch 10, Loss: 0.005174450110644102, Accuracy: 99.79556274414062, Val Loss: 0.14549730718135834, Val Accuracy: 96.69999694824219\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        channel_1, channel_2, num_classes = 28, 14, 10\n",
    "        kernel_initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.conv2d_1 = tf.keras.layers.Conv2D(channel_1, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer)\n",
    "        self.relu_1 = tf.keras.layers.ReLU()\n",
    "        self.conv2d_2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer)\n",
    "        self.relu_2 = tf.keras.layers.ReLU()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=kernel_initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.dense(self.flatten(self.relu_2(self.conv2d_2(self.relu_1(self.conv2d_1(input_tensor))))))\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x\n",
    "\n",
    "\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.065743923187256, Accuracy: 12.5, Val Loss: 2.6301918029785156, Val Accuracy: 9.0\n",
      "Iteration 700, Epoch 1, Loss: 0.5249208211898804, Accuracy: 84.43740844726562, Val Loss: 0.402607798576355, Val Accuracy: 88.70000457763672\n",
      "Iteration 1400, Epoch 2, Loss: 0.28697553277015686, Accuracy: 91.75688934326172, Val Loss: 0.3366349935531616, Val Accuracy: 90.5\n",
      "Iteration 2100, Epoch 3, Loss: 0.2380547821521759, Accuracy: 93.28042602539062, Val Loss: 0.3009321689605713, Val Accuracy: 91.9000015258789\n",
      "Iteration 2800, Epoch 4, Loss: 0.20469968020915985, Accuracy: 94.20663452148438, Val Loss: 0.26400548219680786, Val Accuracy: 93.0999984741211\n",
      "Iteration 3500, Epoch 5, Loss: 0.17488960921764374, Accuracy: 95.05148315429688, Val Loss: 0.24129781126976013, Val Accuracy: 92.4000015258789\n",
      "Iteration 4200, Epoch 6, Loss: 0.15263627469539642, Accuracy: 95.65785217285156, Val Loss: 0.21150705218315125, Val Accuracy: 94.0\n",
      "Iteration 4900, Epoch 7, Loss: 0.13690835237503052, Accuracy: 96.14754486083984, Val Loss: 0.19763675332069397, Val Accuracy: 94.30000305175781\n",
      "Iteration 5600, Epoch 8, Loss: 0.12535308301448822, Accuracy: 96.43043518066406, Val Loss: 0.1828809380531311, Val Accuracy: 94.70000457763672\n",
      "Iteration 6300, Epoch 9, Loss: 0.11099214851856232, Accuracy: 96.875, Val Loss: 0.17216119170188904, Val Accuracy: 94.70000457763672\n",
      "Iteration 7000, Epoch 10, Loss: 0.09750977903604507, Accuracy: 97.37149810791016, Val Loss: 0.16389475762844086, Val Accuracy: 95.30000305175781\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "# Let's try SGD optimizer\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet_drp(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet_drp, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        channel_1, channel_2, num_classes = 28, 14, 10\n",
    "        kernel_initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.conv2d_1 = tf.keras.layers.Conv2D(channel_1, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer)\n",
    "        self.relu_1 = tf.keras.layers.ReLU()\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(rate=0.8)\n",
    "        self.conv2d_2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='same', kernel_initializer=kernel_initializer)\n",
    "        self.relu_2 = tf.keras.layers.ReLU()\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(rate=0.8)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=kernel_initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        x = self.dense(self.flatten(self.dropout_2(self.relu_2(self.conv2d_2(self.dropout_1(self.relu_1(self.conv2d_1(input_tensor))))))))\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\src\\python\\DL_Course\\venv\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 9.975008964538574, Accuracy: 9.375, Val Loss: 2.8528974056243896, Val Accuracy: 11.200000762939453\n",
      "Iteration 700, Epoch 1, Loss: 1.108396053314209, Accuracy: 72.38097381591797, Val Loss: 0.3192019462585449, Val Accuracy: 91.79999542236328\n",
      "Iteration 1400, Epoch 2, Loss: 0.3703233599662781, Accuracy: 88.69832611083984, Val Loss: 0.201957106590271, Val Accuracy: 94.5999984741211\n",
      "Iteration 2100, Epoch 3, Loss: 0.28532084822654724, Accuracy: 91.10281372070312, Val Loss: 0.15183916687965393, Val Accuracy: 95.20000457763672\n",
      "Iteration 2800, Epoch 4, Loss: 0.23482541739940643, Accuracy: 92.74664306640625, Val Loss: 0.13948631286621094, Val Accuracy: 96.20000457763672\n",
      "Iteration 3500, Epoch 5, Loss: 0.21522189676761627, Accuracy: 93.21366882324219, Val Loss: 0.1340184360742569, Val Accuracy: 96.30000305175781\n",
      "Iteration 4200, Epoch 6, Loss: 0.19679488241672516, Accuracy: 93.82581329345703, Val Loss: 0.11862444132566452, Val Accuracy: 96.5\n",
      "Iteration 4900, Epoch 7, Loss: 0.19111619889736176, Accuracy: 94.30840301513672, Val Loss: 0.11554847657680511, Val Accuracy: 96.5\n",
      "Iteration 5600, Epoch 8, Loss: 0.17747879028320312, Accuracy: 94.48875427246094, Val Loss: 0.1088443323969841, Val Accuracy: 96.69999694824219\n",
      "Iteration 6300, Epoch 9, Loss: 0.1665201634168625, Accuracy: 95.01445007324219, Val Loss: 0.10445433855056763, Val Accuracy: 96.4000015258789\n",
      "Iteration 7000, Epoch 10, Loss: 0.1538003385066986, Accuracy: 95.47312927246094, Val Loss: 0.09961596876382828, Val Accuracy: 96.80000305175781\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    return CustomConvNet_drp()\n",
    "\n",
    "# Let's try Adam optimizer and another architecture\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 12.6145658493042, Accuracy: 12.5, Val Loss: 2.848093271255493, Val Accuracy: 13.799999237060547\n",
      "Iteration 700, Epoch 1, Loss: 2.684824228286743, Accuracy: 53.8984489440918, Val Loss: 0.6661323308944702, Val Accuracy: 82.30000305175781\n",
      "Iteration 1400, Epoch 2, Loss: 0.978321373462677, Accuracy: 73.27263641357422, Val Loss: 0.5713334679603577, Val Accuracy: 86.5\n",
      "Iteration 2100, Epoch 3, Loss: 0.76649409532547, Accuracy: 77.86138153076172, Val Loss: 0.5331937074661255, Val Accuracy: 87.69999694824219\n",
      "Iteration 2800, Epoch 4, Loss: 0.6546801328659058, Accuracy: 80.54485321044922, Val Loss: 0.480038046836853, Val Accuracy: 89.0\n",
      "Iteration 3500, Epoch 5, Loss: 0.5788421630859375, Accuracy: 82.76602172851562, Val Loss: 0.42741259932518005, Val Accuracy: 90.5999984741211\n",
      "Iteration 4200, Epoch 6, Loss: 0.5230112671852112, Accuracy: 84.42132568359375, Val Loss: 0.3830554187297821, Val Accuracy: 91.39999389648438\n",
      "Iteration 4900, Epoch 7, Loss: 0.4929662048816681, Accuracy: 85.19979858398438, Val Loss: 0.36044302582740784, Val Accuracy: 91.19999694824219\n",
      "Iteration 5600, Epoch 8, Loss: 0.4699772298336029, Accuracy: 86.0290298461914, Val Loss: 0.34547343850135803, Val Accuracy: 91.0999984741211\n",
      "Iteration 6300, Epoch 9, Loss: 0.4316253364086151, Accuracy: 87.45484161376953, Val Loss: 0.3181743621826172, Val Accuracy: 91.9000015258789\n",
      "Iteration 7000, Epoch 10, Loss: 0.3918638229370117, Accuracy: 87.89427185058594, Val Loss: 0.29419469833374023, Val Accuracy: 92.69999694824219\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    return CustomConvNet_drp()\n",
    "\n",
    "# Let's try Adam optimizer and another architecture\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишите все эксперименты, результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные выводы по проделанной работе:\n",
    "1. Исследованные несколько конфигураций эксперементов показали достойные результаты;\n",
    "2. Говоря, об оптимальной архитектуре - проведенные эксперементы говорят о том, что данные лучше работают с моделью, в которой нет Dropout слоев.\n",
    "3. Сравниавая оптимизаторы, приходим к выводу, что наиболее эффективным является Adam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
